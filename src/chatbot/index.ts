import { ChatOllama } from '@langchain/ollama'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import readline from 'readline/promises';

/**
 * A basic chatbot that continuously takes user input and returns responses generated by an LLM model
 */

const prompt = ChatPromptTemplate.fromTemplate('Tell me a short joke about {input}');
const model = new ChatOllama({
   model: "gemma3",
   temperature: 0,
   maxRetries: 2,
   // verbose: true,
   // other params...
});

const rl = readline.createInterface({
   input: process.stdin,
   output: process.stdout,
});

async function main() {
   let question = 'begin your chat now!';

   while (true) {
      const userPrompt = await rl.question(`${question}\n`);
      const res = await model.invoke(userPrompt);

      question = res.content as string;
   }
}

main();
